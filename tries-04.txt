Pod design 

- Labels And Annotations

Q1 Answer

k run nginx1 --images=nginx --restart=Never --labels=app=v1 // remember --labels=app=v1
k run nginx2 --images=nginx --restart=Never --labels=app=v1
k run nginx3 --images=nginx --restart=Never --labels=app=v1



Q2 Answer

k get po -A --show-labels // wrong
k get po --show-labels // corrected



Q3 Answer

k labels nginx2 app=v2 --overwrite // wrong
k label po nginx2 app=v2 --overwrite // corrected



Q4 Answer

k get po -labels=app // wrong
k get po -L app // corrected



Q5 Answer

k get po -l app=v2



Q6 Answer

k label po -l "app in(v1,v2)" tier=web // corrected



Q7 Answer

k annotate po -l "app=v2" "owner:marketing" // wrong
k annotate po -l "app=v2" owner=marketing // corrected



Q8 Answer

k label -l "app in(v1,v2)"- // wrong
k label po -l app app- // corrected



Q9 Answer

k annotate po nginx1 description='my description'
k annotate po nginx2 description='my description'
k annotate po nginx3 description='my description'

kubectl annotate po nginx1 nginx2 nginx3 description='my description' // better



Q10 Answer

k annotate pod nginx1 --list // corrected - add --list 



Q11 Answer

k annotate nginx1 nginx2 nginx3 description- owner- // corrected also add owner-



Q12 Answer

k delete po nginx1 nginx2 nginx3
kubectl delete po nginx{1..3} // better




- Pod Placements

Q1 Answer

k run nginx --image=nginx --restart=Never 
k label po nginx accelerator=nvidia-tesla-p100 // wrong
k label nodes node_name accelerator=nvidia-tesla-p100 // corrected
k get nodes --show-labels // missing



Q2 Answer

k taint nodes node1 tier=frontend:NoSchedule
// totally forget: Then, create a pod that tolerates this taint.

k describe node node1 // missing 


Q3 Answer

k run nginx --image=nginx --restart=Never // wrong
// totally forget

nano.pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: nginx
    image: nginx
  nodeSelector:
    kubernetes.io/hostname: controlplane
  tolerations:
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Exists"
    effect: "NoSchedule"

node-role.kubernetes.io/control-plane



- Deployments

Q1 Answer

# check --dry-run: k create deploy nginx --image=nginx:1.18.0 --replicas=2 --port=80 --dry-run=client -o yaml
k create deploy nginx --image=nginx:1.18.0 --replicas=2 --port=80



Q2 Answer

k get deploy nginx -o yaml



Q3 Answer

// dont know

k describe deploy nginx

Or 

k get rs -l app=nginx # if you created deployment by 'create' command



Q4 Answer

k get po pod_name -o yaml



Q5 Answer

kubectl rollout status deploy deploy_name



Q6 Answer

kubectl set image deploy nginx nginx=nginx:1.19.8



Q7 Answer

kubectl rollout history deployment nginx
k get deploy nginx // missing
k get rs // missing



Q8 Answer

kubectl rollout undo deploy nginx
k get po // missing



Q9 Answer
# dont know the answer: Do an on-purpose update of the deployment with a wrong image nginx:1.91

k set image deploy nginx nginx=nginx:1.19



Q10 Answer

k rollout status deploy nginx



Q11 Answer

kubectl rollout undo deploy nginx --to-revision=2

k describe deploy nginx



Q12 Answer

kubectl rollout status deploy nginx --to-revision=4 // wrong
kubectl rollout history deploy nginx --to-revision=4 // corrected


Q13 Answer

k scale deploy nginx --replicas=5
kubectl describe deploy nginx // missing



Q14 Answer

# No diea how to do: Autoscale the deployment, pods between 5 and 10, targeting CPU utilization at 80%

kubectl autoscale deployment foo --min=5 --max=10 --cpu-percent=80
kubectl get hpa nginx // missing + dont know



Q15 Answer

k rollout puase deploy nginx 



Q16 Answer

k set image deploy nginx nginx=nginx:1.19.9
k describe deploy nginx
k rollout history deploy nginx // missing



Q17 Answer

k rollout resume deploy nginx 
k describe deploy nginx
k rollout history deploy nginx // missing



Q18 Answer

k delete deploy nginx // wrong
k delete deploy/nginx hpa/nginx // corrected
k get deploy



Q19 Answer

# No idea how todo: Implement canary deployment by running two instances of nginx marked as version=v1 and version=v2 so that the load is balanced at 75%-25% ratio
# Must memo for 1k times !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

k create deploy app-v1 --replicas=3
k create deploy app-v2 --replicas=1
# kubectl create service clusterip my-service --tcp=80:8080 --dry-run=client -o yaml // Forgot
k create svc clusterip app-service --tcp=80:8080


# Eventually

k scale deploy app-v2 --replicas=4 
k delete deploy app-v1

# Canary update done ~



-Jobs

Q1 Answer

k create job pi --image=perl:5.34 -- /bin/sh -c perl -Mbignum=bpi -wle 'print bpi(2000)' // wrong
k create job pi --image=perl:5.34 -- /bin/sh -c "perl -Mbignum=bpi -wle 'print bpi(2000)'" // corrected



Q2 Answer

# No idea how to do? Wait till it's done, get the output

kubectl get jobs -w # wait till 'SUCCESSFUL' is 1 (will take some time, perl image might be big)



Q3 Answer

k create job busybox --image=busybox -- echo hello;sleep 30;echo world



Q4 Answer

ket get po // missing
k logs po busybox-pod-name -f



Q5 Answer

k describe jobs busybox 
k logs job/busybox // missing



Q6 Answer

k delete job busybox



Q7 Answer

# No idea how to do: Create a job but ensure that it will be automatically terminated by kubernetes if it takes more than 30 seconds to execute

kubectl create job busybox --image=busybox --dry-run=client -o yaml -- /bin/sh -c 'while true; do echo hello; sleep 10;done' > job.yaml
vi job.yaml

apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  activeDeadlineSeconds: 30 # add this line
  template:
    metadata:
      creationTimestamp: null
      labels:
        run: busybox
    spec:
      containers:
      - args:
        - /bin/sh
        - -c
        - while true; do echo hello; sleep 10;done
        image: busybox
        name: busybox
        resources: {}
      restartPolicy: OnFailure
status: {}


Please remember:   activeDeadlineSeconds: 30 # add this line
  activeDeadlineSeconds: 30 # add this line
    activeDeadlineSeconds: 30 # add this line



Q8 Answer

kubectl create job busybox --image=busybox --dry-run=client -o yaml > job.yaml
vi job.yaml

apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  completions: 5
  template:
    metadata:
      creationTimestamp: null
      labels:
        run: busybox
    spec:
      containers:
      - image: busybox
        name: busybox
        resources: {}
      restartPolicy: OnFailure
status: {}

kubectl create -f job.yaml



Q9 Answer

kubectl create job busybox --image=busybox --dry-run=client -o yaml > job.yaml
vi job.yaml

apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  parallelism: 5
  template:
    metadata:
      creationTimestamp: null
      labels:
        run: busybox
    spec:
      containers:
      - image: busybox
        name: busybox
        resources: {}
      restartPolicy: OnFailure
status: {}

kubectl create -f job.yaml



- Cron Jobs

Q1 Answer

k create cronjob busybox --image=busybox --schedule="*/1 * * * *" -- /bin/sh -c "date; echo Hello from the Kubernetes cluster"



Q2 Answer

k logs cronjob/busybox -f // wrong
k get po // corrected
k logs pod_name -f // corrected
k delete cronjob busybox


Q3 Answer

k create cronjob busybox --image=busybox --schedule="*/1 * * * *" -- /bin/sh -c "date; echo Hello from the Kubernetes cluster"
k describe cronjob busybox // wrong
k get jobs --watch // corrected
k get po --show-labels // corrected # observe that the pods have a label that mentions their 'parent' job
k logs pod_name -f // please remove for now
k delete cronjob busybox



Q4 Answer

k create cronjob busybox --image=busybox --schedule="*/1 * * * *" -- /bin/sh -c "date; echo Hello from the Kubernetes cluster" -o yaml > cj.yaml
nano cj.yaml

apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "* * * * *"
  jobTemplate:
    spec:
      startingDeadlineSeconds: 17
      template:
        spec:
          containers:
          - name: hello
            image: busybox:1.28
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure


kubectl create -f cj.yaml


Q5 Answer

k create cronjob busybox --image=busybox --schedule="*/1 * * * *" -- /bin/sh -c "date; echo Hello from the Kubernetes cluster" -o yaml > cj.yaml
nano cj.yaml

apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "* * * * *"
  jobTemplate:
    spec:
      startingDeadlineSeconds: 12
      template:
        spec:
          containers:
          - name: hello
            image: busybox:1.28
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure


kubectl create -f cj.yaml



Q6 Answer

kubectl create job busybox --from=cronjob/busybox
