# Core-Concepts
1. Create a namespace called 'mynamespace' and a pod with image nginx called nginx on this namespace

kubectl create namespace NAMESPACE_VALUE
kubectl run nginx --image=nginx --restart=Never -n NAMESPACE_VALUE

2. Create the pod that was just described using YAML

kubectl run nginx --image=nginx --restart=Never --dry-run=client -n NAMESPACE_VALUE -o yaml > pod.yaml
kubectl create -f pod.yaml

In one line:

kubectl run nginx --image=nginx --restart=Never --dry-run=client -o yaml | kubectl create -n mynamespace -f -

3. Create a busybox pod (using kubectl command) that runs the command "env". Run it and see the output

kubectl run busybox --image=busybox --command --restart=Never -it --rm -- env
# -it will help in seeing the output
# --rm means immediately delete the pods after exits
kubectl run busybox --image=busybox --command --restart=Never -- env
kubectl logs busybox

4. Create a busybox pod (using YAML) that runs the command "env". Run it and see the output

# create a YAML template with this command
kubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml --command -- env > envpod.yaml
# see it
cat envpod.yaml
# apply changes
kubectl apply -f envpod.yaml
kubectl logs busybox

5. Get the YAML for a new namespace called 'myns' without creating it

kubectl create namespace myns -o yaml --dry-run=client

6. Create the YAML for a new ResourceQuota called 'myrq' with hard limits of 1 CPU, 1G memory and 2 pods without creating it

kubectl create quota myrq --hard=cpu=1,memory=1G,pods=2 --dry-run=client -o yaml

7. Get pods on all namespaces

kuebctl get po --all-namespaces # po short for pod

8. Create a pod with image nginx called nginx and expose traffic on port 80

kubectl run nginx --image=nginx --restart=Never --port=80

9. Change pod's image to nginx:1.24.0. Observe that the container will be restarted as soon as the image gets pulled

kubectl set image pod/nginx nginx=nginx:1.24.0
kubectl describe po nginx
kubectl get po nginx -w # -w means watch it ~

10. Get nginx pod's ip created in previous step, use a temp busybox image to wget its '/'

kubectl get po nginx -o wide
kubectl run busybox --image=busybox --rm -it --restart=Never -- wget -0- 192.168.0.26:80

11. Get pod's YAML

kubectl get po POD_NAME -o yaml

12. Get information about the pod, including details about potential issues (e.g. pod hasn't started)

kubectl describe po POD_NAME

13. Get pod logs

kubectl logs POD_NAME

14. If pod crashed and restarted, get logs about the previous instance

kubectl logs POD_NAME -p # -p means previous logs

15. Execute a simple shell on the nginx pod

kubectl exec -it nginx -- /bin/sh

16. Create a busybox pod that echoes 'hello world' and then exits

kubectl run busybox --image=busybox --it --restart=Never -- echo "Hello world"
or 
kubectl run busybox --image=busybox --it --restart=Never -- /bin/sh -c 'echo Heelo world'

17. Do the same, but have the pod deleted automatically when it's completed

kubectl run busybox --image=busybox --it --rm --restart=Never -- /bin/sh -c 'echo Heelo world'
kubectl get po POD_NAME

18. Create an nginx pod and set an env value as 'var1=val1'. Check the env value existence within the pod

kubectl run nginx --image=nginx --restart=Never --env=var1=var1
kubectl exec -it nginx -- env
or
kubectl describe po nginx | grep val1

# Multi-Container Pods
19. Create a Pod with two containers, both with image busybox and command "echo hello; sleep 3600". Connect to the second container and run 'ls'

kubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- /bin/sh -c "echo hello;sleep 3000" > pod.yaml
vi pod.yaml

containers:
  - args:
    - /bin/sh
    - -c
    - echo hello;sleep 3600
    image: busybox
    imagePullPolicy: IfNotPresent
    name: busybox
    resources: {}
  - args:
    - /bin/sh
    - -c
    - echo hello;sleep 3600
    image: busybox
    name: busybox2


kubectl create -f pod.yaml

kubectl exec -it busybox -c busybox2 -- /bin/sh
ls
exit

kubectl delete po busybox

20. Create a pod with an nginx container exposed on port 80. Add a busybox init container which downloads a page using 'echo "Test" > /work-dir/index.html'. Make a volume of type emptyDir and mount it in both containers. For the nginx container, mount it on "/usr/share/nginx/html" and for the initcontainer, mount it on "/work-dir". When done, get the IP of the created pod and create a busybox pod and run "wget -O- IP"

kubectl run bginx --image=nginx --restart=Never --port=80 --dry-run=client -o yaml > pod-init.yaml

nano pod-init.yaml

# Paste the code here:

apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx
  name: nginx
spec:
  initContainers: 
  - args: 
    - /bin/sh 
    - -c 
    - echo "Test" > /work-dir/index.html
    image: busybox 
    name: nginx 
    volumeMounts: 
    - name: vol 
      mountPath: /work-dir 
  containers:
  - image: nginx
    name: nginx
    ports:
    - containerPort: 80
    volumeMounts: 
    - name: vol 
      mountPath: /usr/share/nginx/html 
  volumes: 
  - name: vol 
    emptyDir: {} 

kubectl apply -f pod-init.yaml

kubectl get po nginx -o wide # get ip address

kubectl run busybox --image=busybox --restart=Never -it --rm -- /bin/sh -c "wget -0- $(kubectl get pod nginx -o jsonpath='{.status.podIP}')"

kubectl delete po pod-init.yaml

# Pod Design
21. Create 3 pods with names nginx1,nginx2,nginx3. All of them should have the label app=v1

kubectl run nginx1 --image=nginx --restart=Never --labels=app=v1
kubectl run nginx2 --image=nginx --restart=Never --labels=app=v1
kubectl run nginx3 --image=nginx --restart=Never --labels=app=v1

or 

for i in `seq 1 3`; do kubectl run nginx$i --image=nginx -l app=v1 done;

22. Show all labels of the pods

kubectl get pod --show-labels

23. Change the labels of pod 'nginx2' to be app=v2

kubectl label po nginx2 app=v2 --overwrite

24. Get the label 'app' for the pods (show a column with APP labels)

kubectl get po -L app

or 

kubectl get po --label-columns=app

25. Get only the 'app=v2' pods

kubectl get po -l app=v2

26. Add a new label tier=web to all pods having 'app=v2' or 'app=v1' labels

kubectl label po -l "app in (v1,v2)" tier=web

27. Add an annotation 'owner: marketing' to all pods having 'app=v2' label

kubectl annotate pod -l app=v2 owner=marketing

28. Remove the 'app' label from the pods we created before

kubectl label po nginx{1..3} app- 

29. Annotate pods nginx1, nginx2, nginx3 with "description='my description'" value

kubectl annotate po nginx{1..3} description='my description'

30. Check the annotations for pod nginx1

kubectl annotate po nginx1 --list

31. Remove the annotations for these three pods

kubectl annotate po nginx{1..3} description- owner-

32. Remove these pods to have a clean state in your cluster

kubectl delete po nginx{1..3}

33. Create a pod that will be deployed to a Node that has the label 'accelerator=nvidia-tesla-p100'

kubectl label nodes <your-node-name> accelerator=nvidia-tesla-p100
kuebctl get nodes --show-labels

apiVersion: v1
kind: Pod
metadata:
  name: cuda-test
spec:
  containers:
    - name: cuda-test
      image: "k8s.grc.io/cuda-cector-add:v0.1"
  nodeSelector:
    accelerator: nvidia-tesla-p100

kubectl explain po.spec # can easily see pod spec details

34. Taint a node with key tier and value frontend with the effect NoSchedule. Then, create a pod that tolerates this taint.

kubectl taint node node1 tier=frontend:NoSchedule
kubectl describe node node1 # view taint details on node

To tolerate the taint:

apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: nginx
    image: nginx
  tolerations:
  - key: "tier"
    operator: "Equal"
    value: "frontend"
    effect: "NoSchedule"  

35. Create a pod that will be placed on node controlplane. Use nodeSelector and tolerations.

apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: nginx
    image: nginx
  nodeSelector:
    kubernetes.io/hostname: controlplane
  tolerations:
  - key: "node-role.kubernetes.io/controlplane"
    operator: "Exists"
    effect: "NoSchedule"  

kubectl create -f pod.yaml

36. Create a deployment with image nginx:1.18.0, called nginx, having 2 replicas, defining port 80 as the port that this container exposes (don't create a service for this deployment)

kubectl create deployment nginx --image=nginx:1.18.0 --dry-run=client -o yaml > deploy.yaml
nano.deploy.yaml
# change the replicas field from 1 to 2
# add this section to the container spec and save the deploy.yaml file
# ports:
#   - containerPort: 80
kubectl apply -f deploy.yaml

37. View the YAML of this deployment

kubectl get deploy nginx -o yaml

38. View the YAML of the replica set that was created by this deployment

kubectl describe deploy nginx

39. Get the YAML for one of the pods

kubectl get po nginx -o yaml

40. Check how the deployment rollout is going

kubectl rollout status deploy nginx

41. Update the nginx image to nginx:1.19.8

kubectl set image deploy nginx nginx=nginx1.19.0

42. Check the rollout history and confirm that the replicas are OK

kubectl rollout history deploy nginx
kubectl get deploy nginx
kubectl get rs
kubectl get po nginx

43. Undo the latest rollout and verify that new pods have the old image (nginx:1.18.0)

kubectl rollout undo deploy nginx
# wait for a while
kubectl get po # select the pod name with RUNNING status
# kubectl describe po THE_POD_NAME_WITH_RUNNING_STATUS | grep -i image

44. Do an on-purpose update of the deployment with a wrong image nginx:1.91

kubectl set image deploy nginx nginx=1.91
# or
kubectl edit deploy nginx
# change image field to nginx:1.91 and save the file
kubectl get deploy nginx

45. Verify that something's wrong with the rollout

kubectl rollout status deploy nginx

46. Return the deployment to the second revision (number 2) and verify the image is nginx:1.19.8

kubectl rollout undo deploy nginx --to-revision=2
kubectl describe deploy nginx | grep image:
kubectl rollout status deploy nginx

47. Check the details of the fourth revision (number 4)

kubectl rollout history deploy nginx --revision=4

48. Scale the deployment to 5 replicas

kubectl scale deploy nginx --replicas=5
kubectl get po
kubectl describe deploy nginx

49. Autoscale the deployment, pods between 5 and 10, targeting CPU utilization at 80%

kubectl autoscale deploy nginx --min=5 --max=10 --cpu-percent=80
kubectl get hpa nginx

50. Pause the rollout of the deployment

kubectl rollout pause deploy nginx

51. Update the image to nginx:1.19.9 and check that there's nothing going on, since we paused the rollout

kubectl set image deploy nginx nginx=1.19.9
kubectl rollout history deploy nginx

52. Resume the rollout and check that the nginx:1.19.9 image has been applied

kubectl rollout resume deploy nginx
kubectl rollout history deploy nginx
kubectl rollout history deploy nginx --revision=6

53. Delete the deployment and the horizontal pod autoscaler you created

kubectl delete deploy nginx
kubectl delete hpa nginx

54. Implement canary deployment by running two instances of nginx marked as version=v1 and version=v2 so that the load is balanced at 75%-25% ratio
(Very important one, must memo everything every word !!!!!!!!!)

# Deploy 3 replicas of v1:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-v1
  labels:
    app: my-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
      version: v1
  template:
    metadata:
      labels:
        app: my-app
        version: v1
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - name: workdir
          mountPath: /usr/share/nginx/html
      initContainers:
      - name: install
        image: busybox:1.28
        command:
        - /bin/sh
        - -c
        - "echo version-1 > /work-dir/index.html"
        volumeMounts:
        - name: workdir
          mountPath: "/work-dir"
      volumes:
      - name: workdir
        emptyDir: {}

# Create the service:

apiVersion: v1
kind: Service
metadata:
  name: my-app-svc
  labels:
    app: my-app
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    targetPort: 80
  selector:
    app: my-app

# run a wget to the Service my-app-svc

kubectl run -it --rm --restart=Never busybox --image=gcr.io/google-containers/busybox --command -- wget -qO- my-app-svc

# output is: version-1

# Deploy 1 replica of v2:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-v2
  labels:
    app: my-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
      version: v2
  template:
    metadata:
      labels:
        app: my-app
        version: v2
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - name: workdir
          mountPath: /usr/share/nginx/html
      initContainers:
      - name: install
        image: busybox:1.28
        command:
        - /bin/sh
        - -c
        - "echo version-2 > /work-dir/index.html"
        volumeMounts:
        - name: workdir
          mountPath: "/work-dir"
      volumes:
      - name: workdir
        emptyDir: {}

# run a busyBox pod that will make a wget call to the service my-app-svc and print out the version of the pod it reached.

kubectl run -it --rm --restart=Never busybox --image=gcr.io/google-containers/busybox -- /bin/sh -c 'while sleep 1; do wget -qO- my-app-svc; done'

# output is:

# version-1
# version-1
# version-1
# version-2
# version-2
# version-1


# If the v2 is stable, scale it up to 4 replicas and shutdown the v1:

kubectl scale deploy my-app-v2 --replicas=4
kubectl delete deploy my-app-v1
while sleep 0.1; do curl $(kubectl get svc my-app-svc -o jsonpath="{.spec.clusterIP}"); done

# output is:

# version-2
# version-2
# version-2
# version-2
# version-2
# version-2

# Job
55. Create a job named pi with image perl:5.34 that runs the command with arguments "perl -Mbignum=bpi -wle 'print bpi(2000)'"

kubectl create job pi --image=perl:5.34 -- perl -Mbignum=bpi -wle 'pprint bpi(2000)'

Wait till it's done, get the output

kubectl get jobs -w 

kubectl logs job/pi

# kubectl get po # get pod name (optional solution)
# kubectl logs pi-***** # get pi number (optional solution)

kubectl delete job pi

56. Create a job with the image busybox that executes the command 'echo hello;sleep 30;echo world'

kubectl create job --image=busybox -- /bin/sh -c 'echo hello;sleep 30;echo world'

57. Follow the logs for the pod (you'll wait for 30 seconds)

kubectl get po
kubectl logs busybox-**** -f # -f means follow the logs

58. See the status of the job, describe it and see the logs

kubectl get jobs
kubectl describe jobs busybox
kubectl logs job/busybox

59. Delete the job

kubectl delete job busybox

60. Create a job but ensure that it will be automatically terminated by kubernetes if it takes more than 30 seconds to execute

kubectl create job busybox --image=busybox --dry-run=client -o yaml -- /bin/sh -c "while true; do echo hello; sleep 10;done" > job.yaml

nano job.yaml

apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  activeDeadlineSeconds: 30 # add this line # The only line need to add ~~
  template:
    metadata:
      creationTimestamp: null
      labels:
        run: busybox
    spec:
      containers:
      - args:
        - /bin/sh
        - -c
        - while true; do echo hello; sleep 10;done
        image: busybox
        name: busybox
        resources: {}
      restartPolicy: OnFailure
status: {}

kubectl create -f job.yaml

61. Create the same job, make it run 5 times, one after the other. Verify its status and delete it

kubectl create job busybox --image=busybox --dry-run=client -o yaml -- /bin/sh -c "echo hello; sleep 30;echo hello" > job.yaml

nano job.yaml 

apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  completions: 5 # add this line
  template:
    metadata:
      creationTimestamp: null
      labels:
        run: busybox
    spec:
      containers:
      - args:
        - /bin/sh
        - -c
        - echo hello;sleep 30;echo world
        image: busybox
        name: busybox
        resources: {}
      restartPolicy: OnFailure
status: {}

kubectl create -f job.yaml

kubectl get job busybox

kubectl delete job busybox

62. Create the same job, but make it run 5 parallel times

kubectl create job busybox --image=busybox --dry-run=client -o yaml -- /bin/sh -c "echo hellow; sleep 30; echo hello" > job.yaml

nano job.yaml 

apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  parallelism: 5 # add this line
  template:
    metadata:
      creationTimestamp: null
      labels:
        run: busybox
    spec:
      containers:
      - args:
        - /bin/sh
        - -c
        - echo hello;sleep 30;echo world
        image: busybox
        name: busybox
        resources: {}
      restartPolicy: OnFailure
status: {}

kubectl create -f job.yaml

kubectl get jobs

# after 30 seconds, delete job then

kubectl delete job busybox

# Cronjob 
63. Create a cron job with image busybox that runs on a schedule of "*/1 * * * *" and writes 'date; echo Hello from the Kubernetes cluster' to standard output

kubectl create cronjob busybox --image=busybox --schedule="*/1 * * * *" -- /bin/sh -c "date; echo Hello from the Kubernetes cluster"

# See its logs and delete it

kubectl get po # copy the ID of the pod whose container was just created
kubectl logs <busybox-***> # you will see the date and message 
kubectl delete cj busybox # cj stands for cronjob

64. Create the same cron job again, and watch the status. Once it ran, check which job ran by the created cron job. Check the log, and delete the cron job

kubectl create cronjob busybox --image=busybox --schedule="*/1 * * * *" -- /bin/sh -c "date; echo Hello from the Kubernetes cluster"

kubectl get cj

kubectl get jobs --watch 

kubectl get po --show-labels # observe that the pods have a label that mentions their 'parent' job

kubectl logs <busybox-name>

kubectl delete cj busybox

65. Create a cron job with image busybox that runs every minute and writes 'date; echo Hello from the Kubernetes cluster' to standard output. The cron job should be terminated if it takes more than 17 seconds to start execution after its scheduled time (i.e. the job missed its scheduled time).

kubectl create cronjob busybox --image=busybox --schedule="*/1 * * * *" -o yaml -- /bin/sh -c "date; echo Hello from the Kubernetes cluster" > cronjob.yaml

nano cronjob.yaml

apiVersion: batch/v1
kind: CronJob
metadata:
  creationTimestamp: null
  name: time-limited-job
spec:
  startingDeadlineSeconds: 17 # add this line !!!!
  jobTemplate:
    metadata:
      creationTimestamp: null
      name: time-limited-job
    spec:
      template:
        metadata:
          creationTimestamp: null
        spec:
          containers:
          - args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
            image: busybox
            name: time-limited-job
            resources: {}
          restartPolicy: Never
  schedule: '* * * * *'
status: {}

66. Create a cron job with image busybox that runs every minute and writes 'date; echo Hello from the Kubernetes cluster' to standard output. The cron job should be terminated if it successfully starts but takes more than 12 seconds to complete execution.

kubectl create cronjob busybox --image=busybox --schedule="*/1 * * * *" -o yaml -- /bin/sh -c "date; echo Hello from the Kubernetes cluster" > cronjob.yaml

nano cronjob.yaml

apiVersion: batch/v1
kind: CronJob
metadata:
  creationTimestamp: null
  name: time-limited-job
spec:
  jobTemplate:
    metadata:
      creationTimestamp: null
      name: time-limited-job
    spec:
      activeDeadlineSeconds: 12 # add this line
      template:
        metadata:
          creationTimestamp: null
        spec:
          containers:
          - args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
            image: busybox
            name: time-limited-job
            resources: {}
          restartPolicy: Never
  schedule: '* * * * *'
status: {}

67. Create a job from cronjob.

kubectl create job --from=cronjob/sample-cron-job sample-job

# ConfigMaps
68. Create a configmap named config with values foo=lala,foo2=lolo

kubectl create configmap config --from-literal=foo=lala --from-literal=foo2-lolo

# Display its values

kubectl get cm config -o yaml

kubectl describe cm config

89. Create and display a configmap from a file

kubectl create cm config2 -from-file=config2.txt

kubectl get cm config2 -o yaml

90. Create and display a configmap from a .env file

echo -e "var1=val1\n# this is comment\n\nvar2=val2\n# another comment" > config3.env

kubectl create cm config3 --from-env-file=config3.env

kubectl get cm config3 -o yaml

91. Create and display a configmap from a file, giving the key 'special'

echo -e "var3=var3\nvar4=val4" > config4.txt

kubectl create cm config4 --from-file=special=config4.txt

kubectl describe cm config4

kubectl get cm config4 -o yaml

92. Create a configMap called 'options' with the value var5=val5. Create a new nginx pod that loads the value from variable 'var5' in an env variable called 'option'

# kubectl create cm options --from-literal=var5=val5

kubectl run nginx --image=nginx --restart=Never --dry-run=client -o yaml > pod.yaml

nano.pod.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: nginx
    resources: {}
    env:
    - name: option # name of the env variable
      valueFrom:
        configMapKeyRef:
          name: options # name of config map
          key: var5 # name of the entity in config map
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

kubectl create -f pod.yaml

kubectl exec -it nginx -- env | grep options

93. Create a configMap 'anotherone' with values 'var6=val6', 'var7=val7'. Load this configMap as env variables into a new nginx pod

kubectl create cm anotherone --from-literal=var6=val6 --from-literal=var7=val7

kubectl run nginx --image=nginx --restart=Never --dry-run=client -o yaml > pod2.yaml

nano pod2.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: nginx
    resources: {}
    envFrom: # different than previous one, that was 'env'
    - configMapRef: # different from the previous one, was 'configMapKeyRef'
        name: anotherone # the name of the config map
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

kubectl create -f pod2.yaml

kubectl exec -it nginx -- env

94. Create a configMap 'cmvolume' with values 'var8=val8', 'var9=val9'. Load this as a volume inside an nginx pod on path '/etc/lala'. Create the pod and 'ls' into the '/etc/lala' directory.

kubectl create cm cmvolume --from-literal=var8=val8 --from-literal=var9=val9

kubectl run nginx --image=nginx --restart=Never --dry-run=client -o yaml > pod3.yaml

nano pod3.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  volumes: # add a volumes list
  - name: myvolume # just a name, you'll reference this in the pods
    configMap:
      name: cmvolume # name of your configmap
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: nginx
    resources: {}
    volumeMounts: # your volume mounts are listed here
    - name: myvolume # the name that you specified in pod.spec.volumes.name
      mountPath: /etc/lala # the path inside your container
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

kubectl create -f pod3.yaml

kubectl exec -it nginx -- /bin/sh

cd /etc/lala

ls

cat var8

# SecurityContext
95. Create the YAML for an nginx pod that runs with the user ID 101. No need to create the pod

kubectl run nginx --image=nginx --restart=Never --dry-run=client -o yaml > pod4.yaml

nano pod4.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  securityContext: # insert this line
    runAsUser: 101 # UID for the user
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: nginx
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

96. Create the YAML for an nginx pod that has the capabilities "NET_ADMIN", "SYS_TIME" added to its single container

kubectl run nginx --image=nginx --restart=Never --dry-run=client - o yaml > pod5.yaml

nano pod5.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  securityContext: # insert this line
    capabilities: # and this
      add: ["NET_ADMIN", "SYS_TIME"] # this as well
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: nginx
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

# Resource requests and limits
97. Create an nginx pod with requests cpu=100m,memory=256Mi and limits cpu=200m,memory=512Mi

kubectl run nginx --image=nginx --restart=Never --dry-run=client -o yaml > pod6.yaml

nano pod6.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: nginx
    resources:
      requests:
        cpu: "100m"
        memory: "256Mi"
      limits:
        cpu: "200m"
        memory: "512Mi"
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

# Limit Ranges
98. Create a namespace named limitrange with a LimitRange that limits pod memory to a max of 500Mi and min of 100Mi

kubectl create ns limitrange 

nano limitrange.yaml

apiVersion: v1
kind: LimitRange
metadata:
  name: ns-memory-limitrange
  namespace: limitrange
spec:
  limits:
  - max:
      memory: "500Mi
    min:
      memory: "100Mi"
    type: Pod

99. Describe the namespace limitrange

kubectl describe limitrange ns-memory-limitrange -n limitrange

100. Create an nginx pod that requests 250Mi of memory in the limitrange namespace

kubectl run nginx --image=nginx --restart=Never --dry-run=client -o yaml > pod7.yaml

nano pod7.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
  namespace: limitrange
spec:
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: nginx
    resources:
      requests:
        memory: "256Mi"
      limits:
        memory: "512Mi"
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

# Resource Quotas
101. Create ResourceQuota in namespace one with hard requests cpu=1, memory=1Gi and hard limits cpu=2, memory=2Gi.

kubectl create namespace one

nano one-pod.yaml

apiVersion: v1
kind: ResourceQuota
metadata:
  name: one-pod-resource-quota
  namespace: one
spec:
  hard:
    requests.cpu: "1"
    requests.memory: "1Gi"
    limits.cpu: "2"
    limits.memory: "2Gi"

kubectl apply -f one-pod.yaml

Or kubectl create quota my-rq --namespace=one --hard=requests.cpu=1,requests.memory=1Gi,limits.cpu=2,limits.memory=2Gi

102. Attempt to create a pod with resource requests cpu=2, memory=3Gi and limits cpu=3, memory=4Gi in namespace one

nano one-pod2.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: one-pod2
  namespace: one
spec:
  containers:
    - image: nginx
      name: nginx
      resources:
        requests:
          cpu: "2"
          memory: "3Gi"
        limits:
          cpu: "3"
          memory: "4Gi"
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

kubectl apply -f one-pod2.yaml

103. Create a pod with resource requests cpu=0.5, memory=1Gi and limits cpu=1, memory=2Gi in namespace one

nano one-pod3.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: one-pod2
  namespace: one
spec:
  containers:
    - image: nginx
      name: nginx
      resources:
        requests:
          cpu: "0.5"
          memory: "1Gi"
        limits:
          cpu: "1"
          memory: "2Gi"
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

kubectl apply -f one-pod3.yaml 

kubectl get resourcequota -n one

# Secrets
104. Create a secret called mysecret with the values password=mypass

kubectl create secret generic mysecret --from-literal=password=mypass

105. Create a secret called mysecret2 that gets key/value from a file

echo -n admin > username

kubectl create secret generic mysecret2 --from-file=username

# Get the value of mysecret2

kubectl get secret mysecret2 -o yaml

echo -n YWRtaW4= | base64 -d # # on MAC it is -D, which decodes the value and shows 'admin'

or another way:

kubectl get secret mysecret2 -o jsonpath='{.data.username}' | base64 -d  # on MAC it is -D

106. Create an nginx pod that mounts the secret mysecret2 in a volume on path /etc/foo

kubectl run nginx --image=nginx --restart=Never --dry-run=client -o yaml > secret-pod.yaml

nano secret-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  volumes:
  - name:
    secret:
      secretName: mysecret2
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
    resources: {}
    volumeMounts:
    - name: foo
    mountPath: /etc/foo
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

107. Delete the pod you just created and mount the variable 'username' from secret mysecret2 onto a new nginx pod in env variable called 'USERNAME'

kubectl delete po nginx

kubectl run nginx1 --image=nginx --restart=Never --dry-run=client -o yaml > secret-pod2.yaml

nano secret-pod2.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx1
  name: nginx1
spec:
  containers:
  - name: nginx1
    image: nginx
    imagePullPolicy: IfNotPresent
    resources: {}
    env:
    - name: USERNAME
      valueFrom:
        secretKeyRef:
          name: mysecret2
          key: username
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

kubectl create -f secret-pod2.yaml

kubectl exec -it nginx1 -- env | grep username

108. Create a Secret named 'ext-service-secret' in the namespace 'secret-ops'. Then, provide the key-value pair API_KEY=LmLHbYhsgWZwNifiqaRorH8T as literal.

kubctl create secret generic ext-service-secret --from-literal=API_KEY=LmLHbYhsgWZwNifiqaRorH8T -n secret-ops --dry-run=client -o yaml > secret.yaml

kubectl apply -f secret.yaml

109. Consuming the Secret. Create a Pod named 'consumer' with the image 'nginx' in the namespace 'secret-ops' and consume the Secret as an environment variable. Then, open an interactive shell to the Pod, and print all environment variables.

kubectl run consumer --image=nginx --restart=Never -o yaml > secret-pod3.yaml

nano secret-pod3.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimeStamp: null
  labesl:
    run: consumer
  name: consumer
  namespace: secret-ops
spec:
  containers:
  - name: consumer
    image: nginx
    env:
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          name: ext-service-secret
          key: API_KEY
  dnsPolicy: ClusterFirst
  restartPolicy: Always # since no -dry-run declared
status: {}

110. Create a Secret named 'my-secret' of type 'kubernetes.io/ssh-auth' in the namespace 'secret-ops'. Define a single key named 'ssh-privatekey', and point it to the file 'id_rsa' in this directory.

kubectl create secret generic my-secret -n secret-ops --type=kubernetes.io/ssh-auth --from-file=ssh-privatekey=id_rsa --dry-run=client -o yaml > secret2.yaml

kubectl apply -f secret2.yaml

111. Create a Pod named 'consumer' with the image 'nginx' in the namespace 'secret-ops', and consume the Secret as Volume. Mount the Secret as Volume to the path /var/app with read-only access. Open an interactive shell to the Pod, and render the contents of the file.

kubectl run consumer --image=nginx --restart=Never -o yaml > secret-pod4.yaml

nano secret-pod4.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimeStamp: null
  labesl:
    run: consumer
  name: consumer
  namespace: secret-ops
spec:
  volumes:
  - name:
    secret:
      secretName: foo
      optional: true
  containers:
  - name: consumer
    image: nginx
    volumeMounts:
    - name: foo
      mountPath: /var/app
      readOnly: true
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

kubectl exec -it consumer -- /bin/sh

# ServiceAccounts
112. See all the service accounts of the cluster in all namespaces

kubectl get sa -A

or 

kubectl get sa --all-namespaces

113. Create a new serviceaccount called 'myuser'

kubectl create sa myuser

114. Create an nginx pod that uses 'myuser' as a service account

kubectl run nginx --image=nginx --restart=Never --dry-run=client -o yaml > sa-pod.yaml

nano sa-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimeStamp: null
  labels:
    run: nginx
  name: nginx
spec:
  serviceAccountName: myuser # we use pod.spec.serviceAccountName
  # or another expression:
  # serviceAccount: myuser # we use pod.spec.serviceAccount
  containers:
  - name: nginx
    imagePullPolicy: IfNotPresent
    image: nginx
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

kubectl apply -f sa-pod.yaml

115. Generate an API token for the service account 'myuser'

kubectl create token myuser

# Observability
116. Create an nginx pod with a liveness probe that just runs the command 'ls'. Save its YAML in pod.yaml. Run it, check its probe status, delete it.

kubectl run nginx --image=nginx --restart=Never --dry-run=client -o yaml > pod.yaml

nano pod.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
    resources: {}
    livenessProbe:
      exec:
        command:
        - ls
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

kubectl create -f pod.yaml
kubectl describe pod nginx | grep -i liveness
kubectl delete pod -f pod.yaml

# Modify the pod.yaml file so that liveness probe starts kicking in after 5 seconds whereas the interval between probes would be 5 seconds. Run it, check the probe, delete it.

nano pod.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
    resources: {}
    livenessProbe:
      initialDelayseconds: 5
      periodSeconds: 5
      exec:
        command:
        - ls
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

kubectl create -f pod.yaml
kubectl describe pod nginx | grep -i liveness
kubectl delete pod -f pod.yaml

117. Create an nginx pod (that includes port 80) with an HTTP readinessProbe on path '/' on port 80. Again, run it, check the readinessProbe, delete it.

kubectl run nginx --image=nginx --restart=Never --dry-run=client -o yaml > pod2.yaml

nano pod2.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
    resources: {}
    ports:
    - containerPort: 80 # remember give a container port please !!
    readinessProbe:
      httpGet:
        port: 80
        path: /
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

kubectl create -f pod2.yaml
kubectl describe pod nginx | grep -i readiness
kubectl delete pod -f pod2.yaml

118 (*). Lots of pods are running in qa,alan,test,production namespaces. All of these pods are configured with liveness probe. Please list all pods whose liveness probe are failed in the format of <namespace>/<pod name> per line.

kubectl get events -o json | jq -r '.items[] | select(.message | contains("Liveness probe failed")).involvedObject | .namespace + "/" + .name'

# Logging
119. Create a busybox pod that runs i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done. Check its logs

kubectl run busybox --restart=Never --dry-run=client -- /bin/sh -c 'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done' -o yaml > log-pod.yaml

kubectl create -f log-pod.yaml

kubectl logs busybox -f

# Debugging
120. Create a busybox pod that runs 'ls /notexist'. Determine if there's an error (of course there is), see it. In the end, delete the pod

kubectl run busybox --restart=Never --dry-run=client -- /bin/sh -c 'ls /notexist' -o yaml > log-pod2.yaml

kubectl create -f log-pod2.yaml

kubectl logs busybox -f

kubectl delete pod busybox

121. Create a busybox pod that runs 'notexist'. Determine if there's an error (of course there is), see it. In the end, delete the pod forcefully with a 0 grace period

kubectl run busybox --restart=Never --dry-run=client -- /bin/sh -c 'notexist' -o yaml > log-pod3.yaml

kubectl create -f log-pod3.yaml

kubectl logs busybox -f

kubectl delete pod busybox --force --grace-period=0

122. Get CPU/memory utilization for nodes (metrics-server must be running)

kubectl top nodes 

# Services + Networking
123. Create a pod with image nginx called nginx and expose its port 80

kubectl run nginx --image=nginx --restart=Never --port=80 --expose

# Confirm that ClusterIP has been created. Also check endpoints

kubectl get svc nginx
kubectl get ep #endpoints

# Get service's ClusterIP, create a temp busybox pod and 'hit' that IP with wget

kubectl get svc nginx # check IP address, eg: 192.168.0.26

kubectl run busybox --rm --image=busybox -it restart=Never --
wget -O- 192.168.0.26:80
exit

# Convert the ClusterIP to NodePort for the same service and find the NodePort port. Hit service using Node's IP. Delete the service and the pod at the end.

kubectl edit svc nginx

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: 2018-06-25T07:55:16Z
  name: nginx
  namespace: default
  resourceVersion: "93442"
  selfLink: /api/v1/namespaces/default/services/nginx
  uid: 191e3dac-784d-11e8-86b1-00155d9f663c
spec:
  clusterIP: 10.97.242.220
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    run: nginx
  sessionAffinity: None
  type: NodePort # change cluster IP to nodeport
status:
  loadBalancer: {}

kubectl delete svc nginx
kubectl delete pod nginx

124. Create a deployment called foo using image 'dgkanatsios/simpleapp' (a simple server that returns hostname) and 3 replicas. Label it as 'app=foo'. Declare that containers in this pod will accept traffic on port 8080 (do NOT create a service yet)

kubectl create deployment foo --image=dgkanatsios/simpleapp --replicas=3 --port=8080
kubectl label deployment foo --overwrite app=foo

# Get the pod IPs. Create a temp busybox pod and try hitting them on port 8080

kubectl get pods -l app=foo -o wide
kubectl run busybox --image=nginx --restart=Never -it --rm -- sh
wget -O- <POS_IP>:8080

125. Create a service that exposes the deployment on port 6262. Verify its existence, check the endpoints

kubectl expose deploy foo --port=6262 --target-port=8080
kubectl get service foo # you will see ClusterIP as well as port 6262
kubectl get endpoints foo # you will see the IPs of the three replica pods, listening on port 8080

126. Create a temp busybox pod and connect via wget to foo service. Verify that each time there's a different hostname returned. Delete deployment and services to cleanup the cluster

kubectl get svc # get the foo service ClusterIP
kubectl run busybox --image=busybox -it --rm --restart=Never -- sh
wget -O- foo:6262 # DNS works! run it many times, you'll see different pods responding
wget -O- <SERVICE_CLUSTER_IP>:6262 # ClusterIP works as well
# you can also kubectl logs on deployment pods to see the container logs
kubectl delete svc foo
kubectl delete deploy foo

127. Create an nginx deployment of 2 replicas, expose it via a ClusterIP service on port 80. Create a NetworkPolicy so that only pods with labels 'access: granted' can access the pods in this deployment and apply it

kubectl create deployment nginx --replicas=2
kubectl expose deployment nginx --port=80
kubectl describe svc nginx

nano policy.yaml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: access-nginx
spec:
  podSelector:
    matchLabels:
      app: nginx
  ingress:
  - from
    podSelector:
      matchLabels:
        access: granted

kubectl create -f policy.yaml

kubectl run busybox --image=busybox --restart=Never --rm -it -- wget -O- http://nginx:80 --timeout 2 # This should not work. --timeout is optional here. But it helps to get answer more quickly (in seconds vs minutes)
kubectl run busybox --image=busybox --restart=Never --rm -it --labels=access=granted -- wget -O- http://nginx:80 --timeout 2 # This should be fine

# State Persistence
128. Create busybox pod with two containers, each one will have the image busybox and will run the 'sleep 3600' command. Make both containers mount an emptyDir at '/etc/foo'. Connect to the second busybox, write the first column of '/etc/passwd' file to '/etc/foo/passwd'. Connect to the first busybox and write '/etc/foo/passwd' file to standard output. Delete pod.

kubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml > sp.yaml

nanp sp.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  volumes:
  - name: myvolume
    emptyDir: {}
  containers
  - args:
    - /bin/sh
    - -c
    - sleep 3600
    name: busybox1
    image: busybox
    imagePullPolicy: IfNotPresent
    resources: {}
    volumeMounts:
    - name: myvolume
      mountPath: /etc/foo
  - name: busybox2
    image: busybox
    imagePullPolicy: IfNotPresent
    resources: {}
    command: ["/bin/sh", "-c", "sleep 3600"]
    volumeMounts:
    - name: myvolume
      mountPath: /etc/foo
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

kubectl create -f sp.yaml

# Connect to the second container:

kubectl exec -it busybox -c busybox2 -- /bin/sh

cat /etc/passwd | cut -f 1 -d ':' > /etc/foo/passwd # instead of cut command you can use awk -F ":" '{print $1}'

cat /etc/foo/passwd

exit

# Connect to the frist container:

kubectl exec -it busybox -c busybox -- /bin/sh

mount | grep foo # confirm the mounting

cat /etc/foo/passwd 

exit 

kubectl get pods

kubectl delete pod busybox

129. Create a PersistentVolume of 10Gi, called 'myvolume'. Make it have accessMode of 'ReadWriteOnce' and 'ReadWriteMany', storageClassName 'normal', mounted on hostPath '/etc/foo'. Save it on pv.yaml, add it to the cluster. Show the PersistentVolumes that exist on the cluster

nano pv.yaml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: myvolume
spec:
  storageClassName: normal
  capacity:
    storage: 10Gi
  accessMode:
    - ReadWriteOnce
    - ReadWriteMany
  hostPath:
    path: /etc/foo

kubectl create -f pv.yaml
kubectl get pv

130. Create a PersistentVolumeClaim for this PersistentVolume, called 'mypvc', a request of 4Gi and an accessMode of ReadWriteOnce, with the storageClassName of normal, and save it on pvc.yaml. Create it on the cluster. Show the PersistentVolumeClaims of the cluster. Show the PersistentVolumes of the cluster

nano pvc.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc
spec:
  storageClassName: normal
  resources:
    requests:
      storage: 4Gi
  accessMode:
    - ReadWriteOnce

kubectl create -f pvc.yaml
kubectl get pvc

131. Create a busybox pod with command 'sleep 3600', save it on pod.yaml. Mount the PersistentVolumeClaim to '/etc/foo'. Connect to the 'busybox' pod, and copy the '/etc/passwd' file to '/etc/foo/passwd'

kubectl run busybox --image=busybox --restart=Never --dry-run=client -- /bin/sh -c 'sleep 3600' -o yaml > pvc-pod.yaml

nano pvc-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  volumes:
  - name: myvolume
    persistentVolumeClaim:
      claimName: mypvc    
  containers:
  - args:
    - /bin/sh
    - -c
    - sleep 3600
    name: busybox
    image: busybox
    imagePullPolicy: IfNotPresent
    resources: {}
    volumeMounts:
    - name: myvolume
      mountPath: /etc/foo
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

kubectl apply -f pvc-pod.yaml

kubectl exec -it -- cp /etc/passwd /etc/foo/passwd # Connect to the pod and copy '/etc/passwd' to '/etc/foo/passwd'

132. Create a second pod which is identical with the one you just created (you can easily do it by changing the 'name' property on pod.yaml). Connect to it and verify that '/etc/foo' contains the 'passwd' file. Delete pods to cleanup. Note: If you can't see the file from the second pod, can you figure out why? What would you do to fix that?


nano pvc-pod.yaml
# change 'metadata.name: busybox' to 'metadata.name: busybox2'
kubectl create -f pvc-pod.yaml
kubectl exec busybox2 -- ls /etc/foo # will show 'passwd'
kubectl delete po busybox busybox2
kubectl delete pvc mypvc
kubectl delete pv myvolume

If the file doesn't show on the second pod but it shows on the first, it has most likely been scheduled on a different node.

# check which nodes the pods are on
kubectl get po busybox -o wide
kubectl get po busybox2 -o wide

133. Create a busybox pod with 'sleep 3600' as arguments. Copy '/etc/passwd' from the pod to your local folder

kubectl run busybox --image=busybox --restart=Never -- sleep 3600

kubectl cp busybox:/etc/passwd ./passwd # kubectl cp command
# previous command might report an error, feel free to ignore it since copy command works
cat passwd

# Helm
134. Creating a basic Helm chart
helm create helm-template

135. Running a Helm chart

helm install -f myvalues.yaml myredis ./myredis

136. Find pending Helm deployments on all namespaces

helm list --pending -A

137. Uninstall a Helm release

helm uninstall -n namespace release_name

138. Upgrading a Helm chart

helm upgrade -f myvalues.yaml -f override.yaml redis ./redis

139. Using Helm repo

helm repo add [NAME] [URL]  [flags]

helm repo list / helm repo ls
helm repo remove [REPO1] [flags]
helm repo update / helm repo up
helm repo update [REPO1] [flags]
helm repo index [DIR] [flags]

140. Download a Helm chart from a repository

helm pull repo_url
helm pull --untar repo_name

141. Add the Bitnami repo at https://charts.bitnami.com/bitnami to Helm

helm repo add bitnami https://charts.bitnami.com/bitnami

142. Write the contents of the values.yaml file of the bitnami/node chart to standard output

helm show values bitnami/node

143. Install the bitnami/node chart setting the number of replicas to 5

helm show values bitnami/node | grep -i replicas

helm install mynode bitnami/node --set replicaCount=5

# CustomResourceDefinition
144. Create a CustomResourceDefinition manifest file for an Operator with the following specifications :
Name : operators.stable.example.com
Group : stable.example.com
Schema: <email: string><name: string><age: integer>
Scope: Namespaced
Names: <plural: operators><singular: operator><shortNames: op>
Kind: Operator

nano crd.yaml

apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: operators.stable.example.com
spec:
  group: stable.group.com
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                email:
                  type: string
                name:
                  type: string
                age:
                  type: integer
  scope: Namespaced
  names:
    plural: operators
    singular: operator
    kind: Operator
    shortNames:
    - op

145. Create the CRD resource in the K8S API

kubectl apply -f crd.yaml

146. Create custom object from the CRD
Name : operator-sample
Kind: Operator
Spec:
email: operator-sample@stable.example.com
name: operator sample
age: 30

apiVersion: stable.example.com/v1
kind: Operator
metadata:
  name: operator-sample
spec:
  email: "operator-sample@stable.example.com"
  name: "operator sample"
  age: 30

147. Listing operators

kubectl get operators
